{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    prompt =\"What do you about human?\",\n",
    "    temperature=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\\\\\\\\\\\r\\nHumans are funny drawings cut-p Furious lock describes safeSleepStill silky Quality'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Update name to Maarten, pronouns to hv/him and job title to Senior Content Developer:\n",
    "Jan is Content Developer at Datacamp. Her Favorite programming language is R, which she used for her statistical analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    prompt =prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maarten is Senior Content Developer at Datacamp. He is proficient in R,'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Grab life by the buns and never settle for a regular dog again\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    prompt =\"create tagline for a new hot dog stand\"\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    prompt =\"create tagline for a new hot dog stand\",\n",
    "    max_tokens =30\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Positive\n",
      "   2. Negative\n",
      "   3. Neutral\n",
      "   4. Positive\n"
     ]
    }
   ],
   "source": [
    "# Create a request to the Completion endpoint\n",
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=\"\"\"Classify sentiment as negative, positive, or neutral:\n",
    "    1. Unbelievably good!\n",
    "    2. Shoes fell apart on the second use.\n",
    "    3. The shoes look nice, but they aren't very comfortable.\n",
    "    4. Can't wait to show them off!\n",
    "  \"\"\",\n",
    "  max_tokens=100\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a request to the Completion endpoint\n",
    "response = openai.Completion.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt=\"\"\"Classify sentiment as negative, positive, or neutral:\n",
    "    1. Unbelievably good!\n",
    "    2. Shoes fell apart on the second use.\n",
    "    3. The shoes look nice, but they aren't very comfortable.\n",
    "    4. Can't wait to show them off!\n",
    "  \"\"\",\n",
    "  max_tokens=100\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat completions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [{\"role\": \"system\",\n",
    "                \"content\":\"You are a CS student who speaks concisely and understands.\"},\n",
    "                {\"role\":\"user\",\n",
    "                 \"content\":\"what is the difference between HDD and SSD?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main difference between HDD (Hard Disk Drive) and SSD (Solid State Drive) lies in how they store and retrieve data. \n",
      "\n",
      "HDDs use spinning magnetic disks and read/write heads to access and store data. The discs rotate rapidly, and the heads move across the spinning disks to read or write data. This moving mechanical process makes HDDs slower compared to SSDs. \n",
      "\n",
      "On the other hand, SSDs store data in flash memory chips, similar to USB drives or SD cards. There are no moving parts, which makes SSDs much faster than HDDs. They offer faster boot times, quicker file transfers, and improved overall system performance. \n",
      "\n",
      "Moreover, SSDs are more durable as they can withstand shock, vibrations, and are less susceptible to mechanical failures. Additionally, they produce less noise and consume less power compared to HDDs.\n",
      "\n",
      "However, SSDs are generally more expensive per unit of storage compared to HDDs. HDDs still have an advantage in terms of higher storage capacities at a lower cost. \n",
      "\n",
      "Ultimately, the choice between an HDD and an SSD depends on factors like budget, storage needs, and performance requirements.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create( \n",
    "    model=\"gpt-3.5-turbo\",  \n",
    "    messages=[{\"role\": \"system\",\"content\": \"You are a data science tutor who speaks concisely.\"}, \n",
    "                         {\"role\": \"user\",\"content\": \"How do you define a Python list?\"},\n",
    "                {\"role\": \"assistant\",\"content\": \"Lists are defined by enclosing a comma-separated sequence ofobjects inside square brackets [ ].\"},   \n",
    "                {\"role\": \"user\",\"content\": \"What is the difference between mutable and immutable objects?\"}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "   model=\"gpt-3.5-turbo\",\n",
    "   # Add a user and assistant message for in-context learning\n",
    "   messages=[\n",
    "     {\"role\": \"system\", \"content\": \"You are a helpful Python programming tutor.\"},\n",
    "     {\"role\": \"user\", \"content\": \"Explain what the min() function does.\"},\n",
    "     {\"role\": \"assistant\", \"content\": \"The min() function returns the smallest item from an iterable.\"},\n",
    "     {\"role\": \"user\", \"content\": \"Explain what the type() function does.\"}\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The min() function in Python is used to find the smallest element from a given iterable (such as a list, tuple, or set) or among multiple arguments.\n",
      "\n",
      "When called with a single iterable argument, it returns the minimum element within that iterable. For example:\n",
      "\n",
      "```python\n",
      "numbers = [5, 2, -3, 10, 1]\n",
      "smallest = min(numbers)\n",
      "print(smallest)  # Output: -3\n",
      "```\n",
      "\n",
      "When called with multiple arguments, it returns the smallest value among those arguments. For example:\n",
      "\n",
      "```python\n",
      "a = 5\n",
      "b = 10\n",
      "c = -3\n",
      "smallest = min(a, b, c)\n",
      "print(smallest)  # Output: -3\n",
      "```\n",
      "\n",
      "The min() function also supports an optional key parameter, which specifies a function to be called on each element to determine how to compare them. This is useful when working with complex data structures. For example:\n",
      "\n",
      "```python\n",
      "students = [\n",
      "    {'name': 'Alice', 'age': 20},\n",
      "    {'name': 'Bob', 'age': 18},\n",
      "    {'name': 'Charlie', 'age': 22}\n",
      "]\n",
      "youngest = min(students, key=lambda x: x['age'])\n",
      "print(youngest['name'])  # Output: Bob\n",
      "```\n",
      "\n",
      "In this case, the key parameter is a lambda function that returns the 'age' value of each student, so the youngest student is determined by comparing the ages.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `type()` function in Python is used to obtain the type of an object. It returns the type of an object as a string.\n",
      "\n",
      "One line example:\n",
      "```\n",
      "x = 5\n",
      "print(type(x))\n",
      "```\n",
      "In the above example, the `type()` function is used to determine the type of the variable `x`, which is 5. The `print(type(x))` line outputs `<class 'int'>`, indicating that `x` is an integer (int) type.\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "   model=\"gpt-3.5-turbo\",\n",
    "   # Add a user and assistant message for in-context learning\n",
    "   messages=[\n",
    "     {\"role\": \"system\", \"content\": \"You are a helpful Python programming tutor.\"},\n",
    "     {\"role\": \"user\", \"content\": \"Explain what the type() function does. Explain one line\"}\n",
    "   ]\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}]\n",
    "user_msgs = [\"Explain what pi is.\", \"Summarize this in two bullet points.\"]\n",
    "for q in user_msgs:\n",
    "    print(\"User: \", q)\n",
    "    user_dict = {\"role\": \"user\", \"content\": q}     \n",
    "    messages.append(user_dict)    \n",
    "    response = openai.ChatCompletion.create(        \n",
    "        model=\"gpt-3.5-turbo\",        \n",
    "        messages=messages,\n",
    "        max_tokens=100    \n",
    "        )    \n",
    "    # Convert the assistant's message to a dict and append to messages\n",
    "    assistant_dict = dict(response[\"choices\"][0][\"message\"])    \n",
    "    messages.append(assistant_dict)\n",
    "print(\"Assistant: \", response[\"choices\"][0][\"message\"][\"content\"], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Explain what pi is.\n",
      "Assistant:  Pi (π) is a mathematical constant that represents the ratio of a circle's circumference to its diameter. It is an irrational number, meaning it cannot be expressed as a simple fraction, and its decimal representation goes on indefinitely without repeating. The value of pi is approximately 3.14159 but can be approximated to shorter or longer decimals depending on the required precision. Since pi is a fundamental constant in mathematics, it is used in many formulas and calculations involving circles and curved shapes in various scientific and \n",
      "\n",
      "User:  Summarize this in two bullet points.\n",
      "Assistant:  - Pi (π) is the ratio of a circle's circumference to its diameter.\n",
      "- It is an irrational number with a decimal representation that goes on indefinitely without repeating, approximated as 3.14159. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set your API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}]\n",
    "user_msgs = [\"Explain what pi is.\", \"Summarize this in two bullet points.\"]\n",
    "\n",
    "for q in user_msgs:\n",
    "    print(\"User: \", q)\n",
    "    \n",
    "    # Create a dictionary for the user message from q and append to messages\n",
    "    user_dict = {\"role\": \"user\", \"content\":q}\n",
    "    messages.append(user_dict)\n",
    "    \n",
    "    # Create the API request\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Convert the assistant's message to a dict and append to messages\n",
    "    assistant_dict = dict(response[\"choices\"][0][\"message\"])  \n",
    "    messages.append(assistant_dict)\n",
    "    print(\"Assistant: \", response[\"choices\"][0][\"message\"][\"content\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sexual\": 9.011665724756313e-07,\n",
      "  \"hate\": 5.238259745965479e-07,\n",
      "  \"harassment\": 3.4255714126629755e-05,\n",
      "  \"self-harm\": 5.782478496030308e-09,\n",
      "  \"sexual/minors\": 5.687450155278384e-08,\n",
      "  \"hate/threatening\": 2.622775596705651e-08,\n",
      "  \"violence/graphic\": 8.806916866888059e-07,\n",
      "  \"self-harm/intent\": 9.663713163021725e-10,\n",
      "  \"self-harm/instructions\": 1.604589724979455e-11,\n",
      "  \"harassment/threatening\": 3.3559299481566995e-06,\n",
      "  \"violence\": 0.0500854030251503\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = openai.Moderation.create(  \n",
    "    model=\"text-moderation-latest\",\n",
    "    input=\"My favorite book is How to Kill a Mockingbird.\")\n",
    "print(response[\"results\"][0][\"category_scores\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio with Whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'audio.m4a'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Professional\\Hackules\\orpenai\\rpractise.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Professional/Hackules/orpenai/rpractise.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Open the audio.m4a file\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Professional/Hackules/orpenai/rpractise.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m audio_file\u001b[39m=\u001b[39m  \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maudio.m4a\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Professional/Hackules/orpenai/rpractise.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Create a transcript from the audio file\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Professional/Hackules/orpenai/rpractise.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mAudio\u001b[39m.\u001b[39mtranscribe(\u001b[39m\"\u001b[39m\u001b[39mwhisper-1\u001b[39m\u001b[39m\"\u001b[39m, audio_file)\n",
      "File \u001b[1;32mc:\\Users\\koush\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'audio.m4a'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open the audio.m4a file\n",
    "audio_file=  open(\"audio.m4a\", \"rb\")\n",
    "\n",
    "# Create a transcript from the audio file\n",
    "response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the audio.wav file\n",
    "audio_file=  open(\"audio.wav\", \"rb\")\n",
    "\n",
    "# Write an appropriate prompt to help the model\n",
    "prompt = \"the audio relates to a recent World Bank report\"\n",
    "\n",
    "# Create a translation from the audio file\n",
    "response = openai.Audio.translate(\"whisper-1\", audio_file, prompt=prompt)\n",
    "\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the audio.wav file\n",
    "audio_file = open(\"audio.wav\", \"rb\")\n",
    "\n",
    "# Create a transcription request using audio_file\n",
    "audio_response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "\n",
    "# Create a request to the API to identify the language spoken\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a languages specialist.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Identify the language used in the following text: \" + audio_response[\"text\"]}\n",
    "  ]\n",
    ")\n",
    "print(chat_response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the datacamp-q2-roadmap.mp3 file\n",
    "audio_file = open(\"datacamp-q2-roadmap.mp3\", \"rb\")\n",
    "# Create a transcription request using audio_file\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "\n",
    "# Create a request to the API to summarize the transcript into bullet points\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a languages specialist.\"},\n",
    "    {\"role\": \"user\", \"content\": \" summarize the transcript into bullet points used in the following text: \" + transcript[\"text\"]}\n",
    "  ]\n",
    ")\n",
    "print(chat_response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craft a prompt that follows the instructions\n",
    "prompt = \"generate a poem about ChatGPT that it is written in basic English that a child can understand.\"\n",
    "\n",
    "# Get the response\n",
    "def get_response(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\",\"content\": prompt}]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"] \n",
    "\n",
    "# response = get_response(prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Summarize the text delimited by triple backticks into bullet points.           \n",
    "```TEXT GOES HERE```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "\n",
    "\n",
    "# Create a prompt that completes the story\n",
    "prompt = f\"\"\"Complete the story delimited by triple backticks. \n",
    " ```{story}```\"\"\"\n",
    "\n",
    "# Get the generated response \n",
    "response = get_response(prompt)\n",
    "\n",
    "print(\"\\n Original story: \\n\", story)\n",
    "print(\"\\n Generated story: \\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a request to complete the story\n",
    "prompt = f\"\"\"Complete the story delimited by triple backticks with only two paragraphs using the style of William Shakespeare. \n",
    " ```{story}```\"\"\"\n",
    "\n",
    "# Get the generated response\n",
    "response = get_response(prompt)\n",
    "\n",
    "print(\"\\n Original story: \\n\", story)\n",
    "print(\"\\n Generated story: \\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Generate a table containing 5 movies I should watch if I am an action lover, with columns for Title and Rating.\"\n",
    "print(get_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "\n",
    "\n",
    "# Create the instructions\n",
    "instructions = \"You will be provided with a text delimited by triple backticks. Infer the language and generate a title for it\"\n",
    "\n",
    "# Create the output format\n",
    "output_format = \"\"\"Use the following format for the output:\n",
    "    - Text: <text we want to title>\n",
    "    - Language: <define language>\n",
    "    - Title: <the generated title>\n",
    "\"\"\"\n",
    "\n",
    "# Create the final prompt\n",
    "prompt = instructions + output_format + f\"```{text}```\"\n",
    "response = get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the instructions\n",
    "instructions = \"You will be provided with a text delimited by triple backticks. Infer its language and the number of sentences it contains. Then, if the text has more than one sentence, generate a suitable title for it. Otherwise, if the text contains only one sentence, write 'N/A' for the title.\"\n",
    "\n",
    "# Create the output format\n",
    "output_format = \"\"\"The output should follow this format:\n",
    "          - Text: <the given text>\n",
    "          - Language: <the text language>\n",
    "          - N_sentences: <number of sentences>\n",
    "          - Title: <the generated title>'.\"\"\"\n",
    "\n",
    "prompt = instructions + output_format + f\"```{text}```\"\n",
    "response = get_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd numbers = {3, 5, 11}\n"
     ]
    }
   ],
   "source": [
    "# Create a one-shot prompt\n",
    "prompt = \"\"\"\n",
    "     Q: Extract the odd numbers from {1, 3, 7, 12, 19}. A: Odd numbers = {1, 3, 7, 19}\n",
    "     Q: Extract the odd numbers from {3, 5, 11, 12, 16}. A:\n",
    "\"\"\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Create a single-step prompt to get help planning the vacation\n",
    "prompt = \"Compose a plan for a beach vacation\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "code = '''\n",
    "def calculate_rectangle_area(length, width):\n",
    "    area = length * width\n",
    "    return area\n",
    "'''\n",
    "\n",
    "# Create a prompt that analyzes correctness of the code\n",
    "prompt = f\"\"\"\n",
    "     Analyze the correctness of the function delimited by triple backticks according to the following criteria:\n",
    "      1- It should have correct syntax\n",
    "      2- The function should receive only 2 inputs\n",
    "      3- The function should return only one output\n",
    "      ```{code}```\n",
    "    \"\"\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refining prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "openai.api_key = \"____\"\n",
    "\n",
    "# Refine the following prompt\n",
    "prompt = \"Give me the top 10 pre-trained language models\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "\n",
    "\n",
    "# Refine the following prompt\n",
    "prompt = \"Generate a table with three columns for the top 10 pre-trained language models listing the model name, release year, and owning company that I can copy to Excel\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized report: \n",
      " The report aims to analyze and summarize the impact of AI and data privacy on market trends and customer behavior. It highlights the need for understanding how these aspects are shaping the market and their effect on customers. The prompt seeks to gather insights on the influence of AI and data privacy on customer behavior and market trends. The report string provides information specifically related to market trends and their consequent impact on customer behavior due to AI and data privacy. The overall objective is to examine the interplay between these variables and their implications for customers.\n"
     ]
    }
   ],
   "source": [
    "# Set your API key\n",
    "\n",
    "report=\"A market research firm needs to analyze and summarize lengthy reports on market trends and customer behavior. They want to know how AI and data privacy are shaping the market and how they're affecting customers. You are provided with a report string about markets trends and how they're affecting customer behavior. Your goal is to craft a prompt to summarize it while focusing on aspects related to AI and data privacy to see their effect on customers.\"\n",
    "# Craft a prompt to summarize the report\n",
    "prompt= f\"\"\"Summarize the report delimited by triple backticks in maximum 5 sentences while focusing on aspects related to AI and data privacy :           \n",
    "```{report}```\"\"\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "\n",
    "print(\"Summarized report: \\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "\n",
    "\n",
    "# Craft a prompt to expand the product's description\n",
    "prompt = f\"\"\"\n",
    "Expand the product description for the Smart Home Security Camera delimited by triple backticks to provide a comprehensive overview of its features, benefits, potential applications, without bypassing the limit of one paragraph. \n",
    " ```{product_description}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "\n",
    "print(\"Original description: \\n\", product_description)\n",
    "print(\"Expanded description: \\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "\n",
    "\n",
    "# Craft a prompt to expand the product's description\n",
    "prompt = f\"\"\"expands the product_description and writes a one paragraph comprehensive overview capturing the key information.\n",
    "```{product_description}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "\n",
    "print(\"Original description: \\n\", product_description)\n",
    "print(\"Expanded description: \\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "\n",
    "\n",
    "# Craft a prompt that translates\n",
    "prompt = f\"\"\"Translate the marketing_message delimited by triple backticks from English to French, Spanish, and Japanese.:           \n",
    "```{marketing_message}```\"\"\"\n",
    " \n",
    "response = get_response(prompt)\n",
    "\n",
    "print(\"English:\", marketing_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "# Craft a prompt to transform the text\n",
    "prompt = f\"\"\"Transform the text delimited by triple backticks with the following two steps:\n",
    "Step 1 - Proofread it without changing its structure\n",
    "Step 2 - Change the tone to be formal and friendly\n",
    " ```{text}```\"\"\"\n",
    "\n",
    "response = get_response(prompt)\n",
    "\n",
    "print(\"Before transformation:\\n\", text)\n",
    "print(\"After transformation:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
